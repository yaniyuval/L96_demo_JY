
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Using neural networks for L96 parameterization &#8212; Lorenz 1996 two time-scale model for learning machine learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Neural networks" href="gradient_decent.html" />
    <link rel="prev" title="Data Assimilation demo in the Lorenz 96 (L96) two time-scale model" href="DA_demo_L96.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/newlogo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Lorenz 1996 two time-scale model for learning machine learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Lorenz 1996 two time-scale model for learning machine learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Intro
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="presentation-model-setup.html">
   L96 analogs for this project
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Type of Parametrization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="estimating-gcm-parameters.html">
   1. Copy / pasting from gcm-parameterization-problem notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gcm-parameterization-problem.html">
   1. Introducing the need for GCM parameterizations
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Assimilation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DA_demo_L96.html">
   Data Assimilation demo in the Lorenz 96 (L96) two time-scale model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Subgrid Patermetrization
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Using neural networks for L96 parameterization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradient_decent.html">
   Neural networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Different ML Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="random_forest_parameterization.html">
   Starting with a single regression tree
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Implementation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Neural-Network-Advection-FwdEuler.html">
   Using neural networks to parameterize advection in L96
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Neural-Network-Advection.html">
   Using neural networks to parameterize advection in L96
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/Neural_network_for_Lorenz96.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/m2lines/L96_demo"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/m2lines/L96_demo/issues/new?title=Issue%20on%20page%20%2Fnotebooks/Neural_network_for_Lorenz96.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/m2lines/L96_demo/main?urlpath=tree/notebooks/Neural_network_for_Lorenz96.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Using neural networks for L96 parameterization
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-gcm-classes-with-and-without-neural-network-parameterization">
     Create GCM classes with and without neural network parameterization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#getting-training-data-input-output-pairs">
       Getting training data (input output pairs):
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#split-to-train-and-test-validation-data">
       Split to train and test (validation) data:
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-data-loaders">
     Using data loaders
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dataset-and-dataloader-classes-provide-a-very-convenient-way-of-iterating-over-a-dataset-while-training-your-machine-learning-model">
       Dataset and Dataloader classes provide a very convenient way of iterating over a dataset while training your machine learning model.
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#we-need-to-iterate-over-the-data-because-it-is-very-slow-and-memory-intensive-to-hold-all-the-data-and-to-use-gradient-decent-over-all-the-data-simultaneously-see-more-details-here-and-here">
       We need to iterate over the data because it is very slow and memory intensive to hold all the data and to use gradient decent over all the data simultaneously (see more details here and here)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#define-network-structure-in-pytorch">
       Define network structure in pytorch
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#neural-networks-can-have-many-different-structures">
       Neural networks can have many different structures.
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#here-we-will-consider-fully-connected-networks">
         Here we will consider fully connected networks
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#to-undersand-fully-connected-networks-we-only-need-to-understand-linear-regression-and-gradient-descent">
         To undersand fully connected networks, we only need to understand Linear regression (and gradient descent).
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#first-we-will-build-a-linear-regression-network-and-later-see-how-to-generalize-the-linear-regression-in-order-to-use-fully-connected-neural-network">
       First we will build a linear regression ‘network’ and later see how to generalize the linear regression in order to use Fully connected neural network.
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#using-the-nework-to-get-a-prediction">
       Using the nework to get a prediction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#to-adjust-optimize-the-weights-we-need-to-define-a-loss-function">
       To adjust (optimize) the weights we need to define a loss function
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calculating-gradients">
       Calculating gradients
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#updating-the-weights-using-optimizer-basically-built-in-methods-for-optimization-such-as-sgd-adam-and-etc">
       Updating the weights using optimizer (basically built in methods for optimization such as SGD, Adam and etc.)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-effective-value-of-the-gradient-v-at-step-t-in-sgd-with-momentum-beta">
       The  effective value of the gradient (V) at step t in SGD with momentum ($\beta$):
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#v-t-beta-v-t-1-1-beta-nabla-w-l-w-x-y">
       $V_t = \beta V_{t-1} + (1-\beta) \nabla_w L(W,X,y)$
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#and-the-updates-to-the-weights-will-be">
       and the updates to the weights will be:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#w-new-w-old-lr-v-t">
       $w^{new} = w^{old} - LR * V_t$
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-adam-an-adaptive-learning-rate-optimization-algorithm">
     Using - Adam (an adaptive learning rate optimization algorithm)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#adam-is-an-adaptive-learning-rate-method-which-means-it-computes-individual-learning-rates-for-different-parameters">
       Adam is an adaptive learning rate method, which means, it computes individual learning rates for different parameters.
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combining-it-all-together-training-the-whole-network">
     Combining it all together:  training the whole network
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-a-deeper-network-for-the-lorenz96-and-using-non-local-features">
   Using a deeper network  for the Lorenz96  (and using non local features)
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-non-local-train-test-data-sets-8-inputs-8-outputs">
     Create non-local train/test data sets (8 inputs, 8 outputs)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-a-class-of-a-3-layer-fully-connected-network">
     Creating a class of a 3 layer fully-connected network
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-function-relu-a-popular-choice">
     Activation function - ReLU (a popular choice)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-layers-would-contain-only-matrix-multiplication-everything-would-be-linear">
       If layers would contain only matrix multiplication, everything would be linear:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-g-2-layers-of-weight-matrices-a-and-b-x-is-the-input-would-give-a-bx-which-is-linear-in-x">
       e.g., 2 layers of weight matrices  A and B (x is the input) would give $A(Bx)$, which is linear (in x)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#therefore-we-need-to-introduce-some-non-linearity-activation-function">
       Therefore we need to introduce some non-linearity (activation function).
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-neural-network-with-2-layers-of-weight-matrices-a-and-b-is-actually">
       The Neural Network with 2 layers of weight matrices  A and B is actually:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-phi-bx-where-phi-is-an-actication-function">
       $A(\phi(Bx))$ where $\phi$ is an actication function
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#train">
       Train:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-some-more-to-further-improve-performance">
       Training some more to further improve performance
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-and-overfitting">
   Regularization and overfitting
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-methods-are-aimed-to-tackle-overfitting">
     Regularization methods are aimed to tackle overfitting.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#useful-ways-to-think-of-regularization">
     Useful ways to think of regularization:
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#putting-constraints-on-the-model">
       Putting constraints on the model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#adding-a-term-to-the-loss-function-so-that-loss-trainingloss-regularization">
       Adding a term to the loss function so that: Loss = TrainingLoss + Regularization
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-of-neural-networks">
   Regularization of Neural Networks
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weight-decay-l2-norm">
     Weight decay (L2 norm)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#add-a-weight-decay-to-a-network-and-try-to-train-it-again">
     Add a weight decay to a network and try to train it again
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dropout">
       Dropout
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-to-choose-a-learning-rate">
   How to choose a learning rate?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visuzlization-of-the-loss-function">
     Visuzlization of the loss function
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-an-optimal-learning-rate">
     Finding an optimal learning rate
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#we-converged-much-faster-than-before">
       We converged much faster than before.
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#i-won-t-talk-about-but-i-recommend-reading">
     I won’t talk about but I recommend reading:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batchnormalization">
     BatchNormalization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cyclic-learning-rate">
     Cyclic learning rate
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#to-understand-cyclic-learning-rates-and-the-one-cycle-policy-read-more-here">
       To understand cyclic learning rates and the One cycle policy - read more here
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Using neural networks for L96 parameterization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Using neural networks for L96 parameterization
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-gcm-classes-with-and-without-neural-network-parameterization">
     Create GCM classes with and without neural network parameterization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#getting-training-data-input-output-pairs">
       Getting training data (input output pairs):
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#split-to-train-and-test-validation-data">
       Split to train and test (validation) data:
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-data-loaders">
     Using data loaders
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dataset-and-dataloader-classes-provide-a-very-convenient-way-of-iterating-over-a-dataset-while-training-your-machine-learning-model">
       Dataset and Dataloader classes provide a very convenient way of iterating over a dataset while training your machine learning model.
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#we-need-to-iterate-over-the-data-because-it-is-very-slow-and-memory-intensive-to-hold-all-the-data-and-to-use-gradient-decent-over-all-the-data-simultaneously-see-more-details-here-and-here">
       We need to iterate over the data because it is very slow and memory intensive to hold all the data and to use gradient decent over all the data simultaneously (see more details here and here)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#define-network-structure-in-pytorch">
       Define network structure in pytorch
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#neural-networks-can-have-many-different-structures">
       Neural networks can have many different structures.
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#here-we-will-consider-fully-connected-networks">
         Here we will consider fully connected networks
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#to-undersand-fully-connected-networks-we-only-need-to-understand-linear-regression-and-gradient-descent">
         To undersand fully connected networks, we only need to understand Linear regression (and gradient descent).
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#first-we-will-build-a-linear-regression-network-and-later-see-how-to-generalize-the-linear-regression-in-order-to-use-fully-connected-neural-network">
       First we will build a linear regression ‘network’ and later see how to generalize the linear regression in order to use Fully connected neural network.
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#using-the-nework-to-get-a-prediction">
       Using the nework to get a prediction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#to-adjust-optimize-the-weights-we-need-to-define-a-loss-function">
       To adjust (optimize) the weights we need to define a loss function
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calculating-gradients">
       Calculating gradients
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#updating-the-weights-using-optimizer-basically-built-in-methods-for-optimization-such-as-sgd-adam-and-etc">
       Updating the weights using optimizer (basically built in methods for optimization such as SGD, Adam and etc.)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-effective-value-of-the-gradient-v-at-step-t-in-sgd-with-momentum-beta">
       The  effective value of the gradient (V) at step t in SGD with momentum ($\beta$):
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#v-t-beta-v-t-1-1-beta-nabla-w-l-w-x-y">
       $V_t = \beta V_{t-1} + (1-\beta) \nabla_w L(W,X,y)$
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#and-the-updates-to-the-weights-will-be">
       and the updates to the weights will be:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#w-new-w-old-lr-v-t">
       $w^{new} = w^{old} - LR * V_t$
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-adam-an-adaptive-learning-rate-optimization-algorithm">
     Using - Adam (an adaptive learning rate optimization algorithm)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#adam-is-an-adaptive-learning-rate-method-which-means-it-computes-individual-learning-rates-for-different-parameters">
       Adam is an adaptive learning rate method, which means, it computes individual learning rates for different parameters.
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combining-it-all-together-training-the-whole-network">
     Combining it all together:  training the whole network
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-a-deeper-network-for-the-lorenz96-and-using-non-local-features">
   Using a deeper network  for the Lorenz96  (and using non local features)
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-non-local-train-test-data-sets-8-inputs-8-outputs">
     Create non-local train/test data sets (8 inputs, 8 outputs)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-a-class-of-a-3-layer-fully-connected-network">
     Creating a class of a 3 layer fully-connected network
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-function-relu-a-popular-choice">
     Activation function - ReLU (a popular choice)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-layers-would-contain-only-matrix-multiplication-everything-would-be-linear">
       If layers would contain only matrix multiplication, everything would be linear:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-g-2-layers-of-weight-matrices-a-and-b-x-is-the-input-would-give-a-bx-which-is-linear-in-x">
       e.g., 2 layers of weight matrices  A and B (x is the input) would give $A(Bx)$, which is linear (in x)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#therefore-we-need-to-introduce-some-non-linearity-activation-function">
       Therefore we need to introduce some non-linearity (activation function).
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-neural-network-with-2-layers-of-weight-matrices-a-and-b-is-actually">
       The Neural Network with 2 layers of weight matrices  A and B is actually:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-phi-bx-where-phi-is-an-actication-function">
       $A(\phi(Bx))$ where $\phi$ is an actication function
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#train">
       Train:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-some-more-to-further-improve-performance">
       Training some more to further improve performance
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-and-overfitting">
   Regularization and overfitting
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-methods-are-aimed-to-tackle-overfitting">
     Regularization methods are aimed to tackle overfitting.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#useful-ways-to-think-of-regularization">
     Useful ways to think of regularization:
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#putting-constraints-on-the-model">
       Putting constraints on the model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#adding-a-term-to-the-loss-function-so-that-loss-trainingloss-regularization">
       Adding a term to the loss function so that: Loss = TrainingLoss + Regularization
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-of-neural-networks">
   Regularization of Neural Networks
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weight-decay-l2-norm">
     Weight decay (L2 norm)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#add-a-weight-decay-to-a-network-and-try-to-train-it-again">
     Add a weight decay to a network and try to train it again
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dropout">
       Dropout
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-to-choose-a-learning-rate">
   How to choose a learning rate?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visuzlization-of-the-loss-function">
     Visuzlization of the loss function
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-an-optimal-learning-rate">
     Finding an optimal learning rate
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#we-converged-much-faster-than-before">
       We converged much faster than before.
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#i-won-t-talk-about-but-i-recommend-reading">
     I won’t talk about but I recommend reading:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batchnormalization">
     BatchNormalization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cyclic-learning-rate">
     Cyclic learning rate
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#to-understand-cyclic-learning-rates-and-the-one-cycle-policy-read-more-here">
       To understand cyclic learning rates and the One cycle policy - read more here
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="using-neural-networks-for-l96-parameterization">
<h1>Using neural networks for L96 parameterization<a class="headerlink" href="#using-neural-networks-for-l96-parameterization" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">Data</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">torch_lr_finder</span> <span class="kn">import</span> <span class="n">LRFinder</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">14</span><span class="p">)</span>  <span class="c1"># For reproducibility</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">14</span><span class="p">);</span>  <span class="c1"># For reproducibility</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">L96_model</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">L96</span><span class="p">,</span>
    <span class="n">RK2</span><span class="p">,</span>
    <span class="n">RK4</span><span class="p">,</span>
    <span class="n">EulerFwd</span><span class="p">,</span>
    <span class="n">L96_eq1_xdot</span><span class="p">,</span>
    <span class="n">integrate_L96_2t</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://www.ecmwf.int/en/elibrary/10829-predictability-problem-partly-solved">Lorenz (1996)</a> describes a “two time-scale” model in two equations (2 and 3) which are:</p>
<div class="amsmath math notranslate nohighlight" id="equation-aa2dd6f1-77d2-46b4-bf01-7e8c0a2f2d5b">
<span class="eqno">(9)<a class="headerlink" href="#equation-aa2dd6f1-77d2-46b4-bf01-7e8c0a2f2d5b" title="Permalink to this equation">¶</a></span>\[\begin{align}
\frac{d}{dt} X_k
&amp;= - X_{k-1} \left( X_{k-2} - X_{k+1} \right) - X_k + F - \left( \frac{hc}{b} \right) \sum_{j=0}^{J-1} Y_{j,k}
\\
\frac{d}{dt} Y_{j,k}
&amp;= - cbY_{j+1,k} \left( Y_{j+2,k} - X_{j-1,k} \right) - c Y_{j,k} + \frac{hc}{b} X_k
\end{align}\]</div>
<div class="section" id="create-gcm-classes-with-and-without-neural-network-parameterization">
<h2>Create GCM classes with and without neural network parameterization<a class="headerlink" href="#create-gcm-classes-with-and-without-neural-network-parameterization" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">time_method</span> <span class="o">=</span> <span class="n">RK4</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># - a GCM class without any parameterization</span>
<span class="k">class</span> <span class="nc">GCM_no_param</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">time_stepping</span><span class="o">=</span><span class="n">time_method</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">F</span> <span class="o">=</span> <span class="n">F</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_stepping</span> <span class="o">=</span> <span class="n">time_stepping</span>

    <span class="k">def</span> <span class="nf">rhs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">L96_eq1_xdot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">F</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X0</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">nt</span><span class="p">,</span> <span class="n">param</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1"># X0 - initial conditions, dt - time increment, nt - number of forward steps to take</span>
        <span class="c1"># param - parameters of our closure</span>
        <span class="n">time</span><span class="p">,</span> <span class="n">hist</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">dt</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nt</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nt</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X0</span><span class="p">)))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
            <span class="n">X0</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="n">hist</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span>

        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nt</span><span class="p">):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_stepping</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rhs</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
            <span class="n">hist</span><span class="p">[</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">time</span><span class="p">[</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">dt</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hist</span><span class="p">,</span> <span class="n">time</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># - a GCM class including a linear parameterization in rhs of equation for tendency</span>
<span class="k">class</span> <span class="nc">GCM</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">parameterization</span><span class="p">,</span> <span class="n">time_stepping</span><span class="o">=</span><span class="n">time_method</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">F</span> <span class="o">=</span> <span class="n">F</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameterization</span> <span class="o">=</span> <span class="n">parameterization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_stepping</span> <span class="o">=</span> <span class="n">time_stepping</span>

    <span class="k">def</span> <span class="nf">rhs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">L96_eq1_xdot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">F</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameterization</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X0</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">nt</span><span class="p">,</span> <span class="n">param</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1"># X0 - initial conditions, dt - time increment, nt - number of forward steps to take</span>
        <span class="c1"># param - parameters of our closure</span>
        <span class="n">time</span><span class="p">,</span> <span class="n">hist</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">dt</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nt</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nt</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X0</span><span class="p">)))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
            <span class="n">X0</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="n">hist</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span>

        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nt</span><span class="p">):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_stepping</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rhs</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
            <span class="n">hist</span><span class="p">[</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">time</span><span class="p">[</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">dt</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hist</span><span class="p">,</span> <span class="n">time</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># - a GCM class including a neural network parameterization in rhs of equation for tendency</span>
<span class="k">class</span> <span class="nc">GCM_network</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">time_stepping</span><span class="o">=</span><span class="n">time_method</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">F</span> <span class="o">=</span> <span class="n">F</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_stepping</span> <span class="o">=</span> <span class="n">time_stepping</span>

    <span class="k">def</span> <span class="nf">rhs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">linear1</span><span class="o">.</span><span class="n">in_features</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">X_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
            <span class="n">X_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">X_torch</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">L96_eq1_xdot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">F</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">X_torch</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="p">)</span>  <span class="c1"># Adding NN parameterization</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X0</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">nt</span><span class="p">,</span> <span class="n">param</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1"># X0 - initial conditions, dt - time increment, nt - number of forward steps to take</span>
        <span class="c1"># param - parameters of our closure</span>
        <span class="n">time</span><span class="p">,</span> <span class="n">hist</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">dt</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nt</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nt</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X0</span><span class="p">)))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
            <span class="n">X0</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="n">hist</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span>

        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nt</span><span class="p">):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_stepping</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rhs</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
            <span class="n">hist</span><span class="p">[</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">time</span><span class="p">[</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">dt</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hist</span><span class="p">,</span> <span class="n">time</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">time_steps</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">Forcing</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="mi">18</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">time_steps</span>

<span class="c1"># Create a &quot;real world&quot; with K=8 and J=32</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">L96</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">F</span><span class="o">=</span><span class="n">Forcing</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="getting-training-data-input-output-pairs">
<h3>Getting training data (input output pairs):<a class="headerlink" href="#getting-training-data-input-output-pairs" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get training data for the neural network.</span>

<span class="c1"># - Run the true state and output subgrid tendencies (the effect of Y on X is xytrue):</span>
<span class="n">Xtrue</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">xytrue</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_coupling</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">gcm_no_param</span> <span class="o">=</span> <span class="n">GCM_no_param</span><span class="p">(</span><span class="n">Forcing</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="split-to-train-and-test-validation-data">
<h3>Split to train and test (validation) data:<a class="headerlink" href="#split-to-train-and-test-validation-data" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">val_size</span> <span class="o">=</span> <span class="mi">4000</span>  <span class="c1"># number of time steps for validation</span>


<span class="c1"># train:</span>
<span class="n">Xtrue_train</span> <span class="o">=</span> <span class="n">Xtrue</span><span class="p">[</span>
    <span class="p">:</span><span class="o">-</span><span class="n">val_size</span><span class="p">,</span> <span class="p">:</span>
<span class="p">]</span>  <span class="c1"># Flatten because we first use single input as a sample</span>
<span class="n">subgrid_tend_train</span> <span class="o">=</span> <span class="n">xytrue</span><span class="p">[:</span><span class="o">-</span><span class="n">val_size</span><span class="p">,</span> <span class="p">:]</span>

<span class="c1"># test:</span>
<span class="n">Xtrue_test</span> <span class="o">=</span> <span class="n">Xtrue</span><span class="p">[</span><span class="o">-</span><span class="n">val_size</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">subgrid_tend_test</span> <span class="o">=</span> <span class="n">xytrue</span><span class="p">[</span><span class="o">-</span><span class="n">val_size</span><span class="p">:,</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="using-data-loaders">
<h2>Using data loaders<a class="headerlink" href="#using-data-loaders" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li></li>
<li></li>
</ul>
<div class="section" id="dataset-and-dataloader-classes-provide-a-very-convenient-way-of-iterating-over-a-dataset-while-training-your-machine-learning-model">
<h3>Dataset and Dataloader classes provide a very convenient way of iterating over a dataset while training your machine learning model.<a class="headerlink" href="#dataset-and-dataloader-classes-provide-a-very-convenient-way-of-iterating-over-a-dataset-while-training-your-machine-learning-model" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="we-need-to-iterate-over-the-data-because-it-is-very-slow-and-memory-intensive-to-hold-all-the-data-and-to-use-gradient-decent-over-all-the-data-simultaneously-see-more-details-here-and-here">
<h3>We need to iterate over the data because it is very slow and memory intensive to hold all the data and to use gradient decent over all the data simultaneously (see more details <a class="reference external" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/">here</a> and <a class="reference external" href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html">here</a>)<a class="headerlink" href="#we-need-to-iterate-over-the-data-because-it-is-very-slow-and-memory-intensive-to-hold-all-the-data-and-to-use-gradient-decent-over-all-the-data-simultaneously-see-more-details-here-and-here" title="Permalink to this headline">¶</a></h3>
</div>
<!-- This provides a very convenient way of separating the data preparation part from the training procedure.  --><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a data loader</span>

<span class="c1"># Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(</span>
<span class="n">local_torch_dataset</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Xtrue_train</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">subgrid_tend_train</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span>
<span class="p">)</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">1024</span>  <span class="c1"># Number of sample in each batch</span>

<span class="n">loader_local</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">local_torch_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a test dataloader</span>

<span class="n">local_torch_dataset_test</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Xtrue_test</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">subgrid_tend_test</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span>
<span class="p">)</span>

<span class="n">loader_local_test</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">local_torch_dataset_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">loader_local</span><span class="p">)</span>  <span class="c1"># iterating over the data to get one batch</span>
<span class="n">X_iter</span><span class="p">,</span> <span class="n">subgrid_tend_iter</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X_iter</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subgrid_tend_iter</span><span class="p">)</span>

<span class="n">fontsize</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_iter</span><span class="p">,</span> <span class="n">subgrid_tend_iter</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;state&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">fontsize</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;subgrid tendency&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">fontsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([12.9715,  5.3797,  0.8659,  ..., -0.2063, -1.7095, -4.3625],
       dtype=torch.float64)
tensor([-7.5589, -6.6147, -0.8452,  ...,  0.8405,  2.5118,  3.4432],
       dtype=torch.float64)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;subgrid tendency&#39;)
</pre></div>
</div>
<img alt="../_images/Neural_network_for_Lorenz96_17_2.png" src="../_images/Neural_network_for_Lorenz96_17_2.png" />
</div>
</div>
<div class="section" id="define-network-structure-in-pytorch">
<h3>Define network structure in pytorch<a class="headerlink" href="#define-network-structure-in-pytorch" title="Permalink to this headline">¶</a></h3>
<p>If you want to learn more:</p>
<p>https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html</p>
<p>https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</p>
<ul class="simple">
<li></li>
<li></li>
<li></li>
</ul>
<p><img
src="https://miro.medium.com/max/720/1*VHOUViL8dHGfvxCsswPv-Q.png" width=400></p>
</div>
<div class="section" id="neural-networks-can-have-many-different-structures">
<h3>Neural networks can have many different structures.<a class="headerlink" href="#neural-networks-can-have-many-different-structures" title="Permalink to this headline">¶</a></h3>
<div class="section" id="here-we-will-consider-fully-connected-networks">
<h4>Here we will consider fully connected networks<a class="headerlink" href="#here-we-will-consider-fully-connected-networks" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="to-undersand-fully-connected-networks-we-only-need-to-understand-linear-regression-and-gradient-descent">
<h4>To undersand fully connected networks, we only need to understand Linear regression (and gradient descent).<a class="headerlink" href="#to-undersand-fully-connected-networks-we-only-need-to-understand-linear-regression-and-gradient-descent" title="Permalink to this headline">¶</a></h4>
</div>
</div>
<div class="section" id="first-we-will-build-a-linear-regression-network-and-later-see-how-to-generalize-the-linear-regression-in-order-to-use-fully-connected-neural-network">
<h3>First we will build a linear regression ‘network’ and later see how to generalize the linear regression in order to use Fully connected neural network.<a class="headerlink" href="#first-we-will-build-a-linear-regression-network-and-later-see-how-to-generalize-the-linear-regression-in-order-to-use-fully-connected-neural-network" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define a network structure in pytorch (here it is a linear network)</span>
<span class="k">class</span> <span class="nc">linear_reg</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">linear_reg</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># a single input and a single output</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span>
    <span class="p">):</span>  <span class="c1"># when calling the model (&#39;linear_reg(input)&#39;) it calls automatically the forward method we defined (via __call__ - see https://github.com/pytorch/pytorch/blob/472be69a736c0b2aece4883be9f8b18e2f3dfbbd/torch/nn/modules/module.py#L487)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lin_net</span> <span class="o">=</span> <span class="n">linear_reg</span><span class="p">()</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lin_net</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>linear_reg(
  (linear1): Linear(in_features=1, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="using-the-nework-to-get-a-prediction">
<h3>Using the nework to get a prediction<a class="headerlink" href="#using-the-nework-to-get-a-prediction" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># An example of how to plug a sample into the network</span>
<span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">lin_net</span><span class="p">(</span><span class="n">input1</span><span class="p">)</span>
<span class="c1"># when calling the model (&#39;lin_net(input)&#39;) it calls automatically the forward method we defined (via __call__ - see https://github.com/pytorch/pytorch/blob/472be69a736c0b2aece4883be9f8b18e2f3dfbbd/torch/nn/modules/module.py#L487)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The output of the random input is:&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The output of the random input is: [[-0.05858676]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="to-adjust-optimize-the-weights-we-need-to-define-a-loss-function">
<h3>To adjust (optimize) the weights we need to define a loss function<a class="headerlink" href="#to-adjust-optimize-the-weights-we-need-to-define-a-loss-function" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>  <span class="c1"># MSE loss function</span>
<span class="n">X_tmp</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">loader_local</span><span class="p">))</span>

<span class="n">y_tmp</span> <span class="o">=</span> <span class="n">lin_net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">X_tmp</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># Predict</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_tmp</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">X_tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># calculate the MSE loss loss</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="calculating-gradients">
<h3>Calculating gradients<a class="headerlink" href="#calculating-gradients" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lin_net</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># zeroes the gradient buffers of all parameters</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;conv1.bias.grad before backward&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lin_net</span><span class="o">.</span><span class="n">linear1</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>

<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
    <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>  <span class="c1"># Computes the gradient of all components current tensor</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;conv1.bias.grad after backward&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lin_net</span><span class="o">.</span><span class="n">linear1</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>conv1.bias.grad before backward
None
conv1.bias.grad after backward
tensor([14.0364], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="updating-the-weights-using-optimizer-basically-built-in-methods-for-optimization-such-as-sgd-adam-and-etc">
<h3>Updating the weights using optimizer (basically built in methods for optimization such as SGD, Adam and etc.)<a class="headerlink" href="#updating-the-weights-using-optimizer-basically-built-in-methods-for-optimization-such-as-sgd-adam-and-etc" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lin_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.003</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before backward pass: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">lin_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After backward pass: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">lin_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before backward pass: 
 [[0.65897453]]
After backward pass: 
 [[-0.06057031]]
</pre></div>
</div>
</div>
</div>
<p>it’s crucial you choose the correct learning rate as otherwise your network will either fail to train, or take much longer to converge. <a class="reference external" href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d">Here</a> you can read more about the momentum term in SGD.</p>
</div>
<div class="section" id="the-effective-value-of-the-gradient-v-at-step-t-in-sgd-with-momentum-beta">
<h3>The  effective value of the gradient (V) at step t in SGD with momentum ($\beta$):<a class="headerlink" href="#the-effective-value-of-the-gradient-v-at-step-t-in-sgd-with-momentum-beta" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="v-t-beta-v-t-1-1-beta-nabla-w-l-w-x-y">
<h3>$V_t = \beta V_{t-1} + (1-\beta) \nabla_w L(W,X,y)$<a class="headerlink" href="#v-t-beta-v-t-1-1-beta-nabla-w-l-w-x-y" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="and-the-updates-to-the-weights-will-be">
<h3>and the updates to the weights will be:<a class="headerlink" href="#and-the-updates-to-the-weights-will-be" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="w-new-w-old-lr-v-t">
<h3>$w^{new} = w^{old} - LR * V_t$<a class="headerlink" href="#w-new-w-old-lr-v-t" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="using-adam-an-adaptive-learning-rate-optimization-algorithm">
<h2>Using - Adam (an adaptive learning rate optimization algorithm)<a class="headerlink" href="#using-adam-an-adaptive-learning-rate-optimization-algorithm" title="Permalink to this headline">¶</a></h2>
<p>The choice of which optimizer we choose might be very important. It will determine how fast the network will be able to learn. Adam is a very popular choice (read more <a class="reference external" href="https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c">here</a> about Adam, and <a class="reference external" href="https://ruder.io/optimizing-gradient-descent/index.html#adam">here</a> about many types of different optimizers).</p>
<div class="section" id="adam-is-an-adaptive-learning-rate-method-which-means-it-computes-individual-learning-rates-for-different-parameters">
<h3>Adam is an adaptive learning rate method, which means, it computes individual learning rates for different parameters.<a class="headerlink" href="#adam-is-an-adaptive-learning-rate-method-which-means-it-computes-individual-learning-rates-for-different-parameters" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="combining-it-all-together-training-the-whole-network">
<h2>Combining it all together:  training the whole network<a class="headerlink" href="#combining-it-all-together-training-the-whole-network" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">trainloader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>  <span class="c1"># for each training step</span>
        <span class="n">b_x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>  <span class="c1"># Inputs</span>
        <span class="n">b_y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span>  <span class="c1"># outputs</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">b_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">):</span>  <span class="c1"># If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
                <span class="n">net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">b_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="p">)</span>  <span class="c1"># input x and predict based on x</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">b_x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">b_y</span><span class="p">)</span>  <span class="c1"># Calculating loss</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># clear gradients for next train</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># backpropagation, compute gradients</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># apply gradients to update weights</span>


<span class="c1">#         test_loss = test_loss + loss.data.numpy() # Keep track of the loss for convenience</span>
<span class="c1">#     test_loss /= len(trainloader) # dividing by the number of batches</span>
<span class="c1">#     print(&#39;the loss in this Epoch&#39;,test_loss)</span>
<span class="c1">#     print(b_y.shape)</span>
<span class="c1">#     print(prediction.shape)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">trainloader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">):</span>
    <span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Evaluation mode (important when having dropout layers)</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="n">trainloader</span>
        <span class="p">):</span>  <span class="c1"># for each training step</span>
            <span class="n">b_x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>  <span class="c1"># Inputs</span>
            <span class="n">b_y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span>  <span class="c1"># outputs</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">b_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="p">):</span>  <span class="c1"># If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)</span>
                <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
                    <span class="n">net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">b_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="p">)</span>  <span class="c1"># input x and predict based on x</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">prediction</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">b_x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">b_y</span><span class="p">)</span>  <span class="c1"># Calculating loss</span>
            <span class="n">test_loss</span> <span class="o">=</span> <span class="n">test_loss</span> <span class="o">+</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Keep track of the loss</span>
        <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span>  <span class="c1"># dividing by the number of batches</span>
        <span class="c1">#         print(len(trainloader))</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">text</span> <span class="o">+</span> <span class="s2">&quot; loss:&quot;</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">test_loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Number of epocs (the number of times we iterate over the training data during training)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
    <span class="n">lin_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.03</span>
<span class="p">)</span>  <span class="c1"># If we have time we can discuss later the Adam optimizer</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train_model</span><span class="p">(</span><span class="n">lin_net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader_local</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">test_model</span><span class="p">(</span><span class="n">lin_net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader_local</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="n">test_model</span><span class="p">(</span><span class="n">lin_net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader_local_test</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 4.01076766397729
validation loss: 4.021152039673325
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 4.003080934482223
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>validation loss: 4.003328110206599
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 3.9984006864662147
validation loss: 4.0310886067231575
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">lin_net</span><span class="o">.</span><span class="n">linear1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lin_net</span><span class="o">.</span><span class="n">linear1</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[-0.85209807]]
[-0.77366882]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds22</span> <span class="o">=</span> <span class="n">lin_net</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Xtrue_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">preds22</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1000</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicted values&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">subgrid_tend_test</span><span class="p">[:</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True values&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neural_network_for_Lorenz96_39_0.png" src="../_images/Neural_network_for_Lorenz96_39_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">T_test</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">X_full</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">T_test</span><span class="p">)</span>  <span class="c1"># Full model</span>


<span class="n">init_cond</span> <span class="o">=</span> <span class="n">Xtrue</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

<span class="n">gcm_net</span> <span class="o">=</span> <span class="n">GCM_network</span><span class="p">(</span><span class="n">Forcing</span><span class="p">,</span> <span class="n">lin_net</span><span class="p">)</span>
<span class="n">Xnn_1layer</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">gcm_net</span><span class="p">(</span><span class="n">init_cond</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">T_test</span> <span class="o">/</span> <span class="n">dt</span><span class="p">),</span> <span class="n">lin_net</span><span class="p">)</span>

<span class="n">gcm_no_param</span> <span class="o">=</span> <span class="n">GCM_no_param</span><span class="p">(</span><span class="n">Forcing</span><span class="p">)</span>
<span class="n">X_no_param</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">gcm_no_param</span><span class="p">(</span><span class="n">init_cond</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">T_test</span> <span class="o">/</span> <span class="n">dt</span><span class="p">))</span>


<span class="n">naive_parameterization</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">param</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">gcm</span> <span class="o">=</span> <span class="n">GCM</span><span class="p">(</span><span class="n">Forcing</span><span class="p">,</span> <span class="n">naive_parameterization</span><span class="p">)</span>
<span class="n">X_param</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">gcm</span><span class="p">(</span><span class="n">init_cond</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">T</span> <span class="o">/</span> <span class="n">dt</span><span class="p">),</span> <span class="n">param</span><span class="o">=</span><span class="p">[</span><span class="mf">0.85439536</span><span class="p">,</span> <span class="mf">1.75218026</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">time_i</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[:</span><span class="n">time_i</span><span class="p">],</span> <span class="n">X_full</span><span class="p">[:</span><span class="n">time_i</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Full L96&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[:</span><span class="n">time_i</span><span class="p">],</span> <span class="n">Xnn_1layer</span><span class="p">[:</span><span class="n">time_i</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;NN 1 layer&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[:</span><span class="n">time_i</span><span class="p">],</span> <span class="n">X_no_param</span><span class="p">[:</span><span class="n">time_i</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;no parameterization&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[:</span><span class="n">time_i</span><span class="p">],</span> <span class="n">X_param</span><span class="p">[:</span><span class="n">time_i</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;linear param (previously used)&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neural_network_for_Lorenz96_41_0.png" src="../_images/Neural_network_for_Lorenz96_41_0.png" />
</div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="using-a-deeper-network-for-the-lorenz96-and-using-non-local-features">
<h1>Using a deeper network  for the Lorenz96  (and using non local features)<a class="headerlink" href="#using-a-deeper-network-for-the-lorenz96-and-using-non-local-features" title="Permalink to this headline">¶</a></h1>
<p><img src="https://www.researchgate.net/publication/319201436/figure/fig1/AS:869115023589376@1584224577926/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values.png" width=400> <em>Fig. 1: Visualisation of a two-scale Lorenz ‘96 system with J = 8 and K = 6. Global-scale values (X k ) are updated based on neighbouring values and a reduction applied to the local-scale values (Y j,k ) associated with that value. Local-scale values are updated based on neighbouring values and the associated global-scale value. The neighbourhood topology of both the local and global-scale values is circular. Image from <a class="reference external" href="https://www.researchgate.net/figure/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values_fig1_319201436">Exploiting the chaotic behaviour of atmospheric models with reconfigurable architectures - Scientific Figure on ResearchGate.</a></em></p>
<div class="section" id="create-non-local-train-test-data-sets-8-inputs-8-outputs">
<h2>Create non-local train/test data sets (8 inputs, 8 outputs)<a class="headerlink" href="#create-non-local-train-test-data-sets-8-inputs-8-outputs" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create non local training data</span>
<span class="c1"># Define a data loader (8 inputs, 8 outputs)</span>

<span class="c1"># Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(</span>
<span class="n">torch_dataset</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Xtrue_train</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">subgrid_tend_train</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span>
<span class="p">)</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">1024</span>  <span class="c1"># Number of sample in each batch</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">torch_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a test dataloader (8 inputs, 8 outputs)</span>

<span class="n">torch_dataset_test</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Xtrue_test</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">subgrid_tend_test</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">loader_test</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">torch_dataset_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="creating-a-class-of-a-3-layer-fully-connected-network">
<h2>Creating a class of a 3 layer fully-connected network<a class="headerlink" href="#creating-a-class-of-a-3-layer-fully-connected-network" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define network structure in pytorch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">FF</span>


<span class="k">class</span> <span class="nc">Net_ANN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net_ANN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>  <span class="c1"># 8 inputs, 16 neurons for first hidden layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>  <span class="c1"># 16 neurons for second hidden layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># 8 outputs</span>

    <span class="c1">#         self.lin_drop = nn.Dropout(0.1) #regularization method to prevent overfitting.</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">FF</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">FF</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="activation-function-relu-a-popular-choice">
<h2>Activation function - ReLU (a popular choice)<a class="headerlink" href="#activation-function-relu-a-popular-choice" title="Permalink to this headline">¶</a></h2>
<div class="section" id="if-layers-would-contain-only-matrix-multiplication-everything-would-be-linear">
<h3>If layers would contain only matrix multiplication, everything would be linear:<a class="headerlink" href="#if-layers-would-contain-only-matrix-multiplication-everything-would-be-linear" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li></li>
<li></li>
<li></li>
</ul>
</div>
<div class="section" id="e-g-2-layers-of-weight-matrices-a-and-b-x-is-the-input-would-give-a-bx-which-is-linear-in-x">
<h3>e.g., 2 layers of weight matrices  A and B (x is the input) would give $A(Bx)$, which is linear (in x)<a class="headerlink" href="#e-g-2-layers-of-weight-matrices-a-and-b-x-is-the-input-would-give-a-bx-which-is-linear-in-x" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="therefore-we-need-to-introduce-some-non-linearity-activation-function">
<h3>Therefore we need to introduce some non-linearity (activation function).<a class="headerlink" href="#therefore-we-need-to-introduce-some-non-linearity-activation-function" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="the-neural-network-with-2-layers-of-weight-matrices-a-and-b-is-actually">
<h3>The Neural Network with 2 layers of weight matrices  A and B is actually:<a class="headerlink" href="#the-neural-network-with-2-layers-of-weight-matrices-a-and-b-is-actually" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="a-phi-bx-where-phi-is-an-actication-function">
<h3>$A(\phi(Bx))$ where $\phi$ is an actication function<a class="headerlink" href="#a-phi-bx-where-phi-is-an-actication-function" title="Permalink to this headline">¶</a></h3>
<p>The ReLu ativation function is just the max(0,X) - and this what is enabling the NN to be a nonlinear function of the inputs!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;ReLU&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;ReLU&#39;)
</pre></div>
</div>
<img alt="../_images/Neural_network_for_Lorenz96_52_1.png" src="../_images/Neural_network_for_Lorenz96_52_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">14</span><span class="p">)</span>  <span class="c1"># For reproducibility</span>
<span class="n">nn_3l</span> <span class="o">=</span> <span class="n">Net_ANN</span><span class="p">()</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="train">
<h3>Train:<a class="headerlink" href="#train" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># Number of epocs</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">nn_3l</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.003</span><span class="p">)</span>
<span class="n">validation_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="c1"># time0 = time()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train_model</span><span class="p">(</span><span class="n">nn_3l</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">))</span>
    <span class="n">validation_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader_test</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">validation_loss</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation loss&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 30.11752312633508
validation loss: 30.118915290524914
train loss: 19.44188452003369
validation loss: 19.567465155549137
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 12.673455156588787
validation loss: 12.72573836248402
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 8.981023541039761
validation loss: 8.969868011101731
train loss: 7.362135832182051
validation loss: 7.296006627695814
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 6.381767226898048
validation loss: 6.366601982835725
train loss: 5.558710861416543
validation loss: 5.587130341393454
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 4.812780326435144
validation loss: 4.862176965081709
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 4.23814587100924
validation loss: 4.257574305874837
train loss: 3.8931013768725515
validation loss: 3.9217922965884195
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 3.694052399305309
validation loss: 3.6956443625001176
train loss: 3.52405161320218
validation loss: 3.5330761472980696
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 3.3591067958120195
validation loss: 3.352024248938528
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 3.187602301786935
validation loss: 3.1667010885303943
train loss: 3.047532931173087
validation loss: 3.0101967712641526
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.944460729527875
validation loss: 2.912183445374992
train loss: 2.8799338816606554
validation loss: 2.8460628369540966
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.838346619257628
validation loss: 2.801685128887617
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.8051665516835773
validation loss: 2.762506002014553
train loss: 2.782228550938645
validation loss: 2.7278937357954227
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.753022763328556
validation loss: 2.7054397176651985
train loss: 2.7320750277068604
validation loss: 2.6929349948085712
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.715879991611459
validation loss: 2.657970802398177
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.7003958680967663
validation loss: 2.640571809321296
train loss: 2.685637743654191
validation loss: 2.627313979835866
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.6638737892819737
validation loss: 2.6151054896122217
train loss: 2.652343404967359
validation loss: 2.6037022389450173
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.6370659467170627
validation loss: 2.5820661898720765
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.6231860712089237
validation loss: 2.578107164629894
train loss: 2.6155459691546956
validation loss: 2.574426232391399
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.603092812587362
validation loss: 2.564002499837138
train loss: 2.5973341212300465
validation loss: 2.5422291587525403
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.5854734335247906
validation loss: 2.534587076692971
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.5774023774964196
validation loss: 2.536986916998388
train loss: 2.5540646085260645
validation loss: 2.5151112992986198
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.5609394899646847
validation loss: 2.5079609432472307
train loss: 2.54223911858353
validation loss: 2.5030648191402176
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.53051602877902
validation loss: 2.487952557358665
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.5276674647137933
validation loss: 2.5135119416359366
train loss: 2.513855857196844
validation loss: 2.488129081876798
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.503379144873901
validation loss: 2.472402509165546
train loss: 2.5025975309751725
validation loss: 2.475460231317543
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.4936068466382855
validation loss: 2.4531874523409836
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.4805978975745306
validation loss: 2.453262146686764
train loss: 2.4749361708613353
validation loss: 2.4467389212641466
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.47152368112684
validation loss: 2.4364656739260946
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.4589943382976376
validation loss: 2.4327517658263944
train loss: 2.4579898827466824
validation loss: 2.43055390960033
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.455973916250345
validation loss: 2.434120381955682
train loss: 2.444197501459908
validation loss: 2.4206268797585704
</pre></div>
</div>
<img alt="../_images/Neural_network_for_Lorenz96_55_30.png" src="../_images/Neural_network_for_Lorenz96_55_30.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds22</span> <span class="o">=</span> <span class="n">nn_3l</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Xtrue_test</span><span class="p">[:,</span> <span class="p">:])</span><span class="o">.</span><span class="n">double</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">preds22</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;NN Predicted values&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">subgrid_tend_test</span><span class="p">[:</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True values&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neural_network_for_Lorenz96_56_0.png" src="../_images/Neural_network_for_Lorenz96_56_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">T_test</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># X_full,_,_ = W.run(dt, T_test) # Full model</span>

<span class="n">gcm_net_3layers</span> <span class="o">=</span> <span class="n">GCM_network</span><span class="p">(</span><span class="n">Forcing</span><span class="p">,</span> <span class="n">nn_3l</span><span class="p">)</span>
<span class="n">Xnn_3layer</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">gcm_net_3layers</span><span class="p">(</span><span class="n">init_cond</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">T_test</span> <span class="o">/</span> <span class="n">dt</span><span class="p">),</span> <span class="n">nn_3l</span><span class="p">)</span>

<span class="n">gcm_net_1layers</span> <span class="o">=</span> <span class="n">GCM_network</span><span class="p">(</span><span class="n">Forcing</span><span class="p">,</span> <span class="n">lin_net</span><span class="p">)</span>
<span class="n">Xnn_1layer</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">gcm_net_1layers</span><span class="p">(</span><span class="n">init_cond</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">T_test</span> <span class="o">/</span> <span class="n">dt</span><span class="p">),</span> <span class="n">lin_net</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">time_i</span> <span class="o">=</span> <span class="mi">240</span>
<span class="n">channel</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[:</span><span class="n">time_i</span><span class="p">],</span> <span class="n">X_full</span><span class="p">[:</span><span class="n">time_i</span><span class="p">,</span> <span class="n">channel</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Full L96&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[:</span><span class="n">time_i</span><span class="p">],</span> <span class="n">Xnn_1layer</span><span class="p">[:</span><span class="n">time_i</span><span class="p">,</span> <span class="n">channel</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;NN 1 layer&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[:</span><span class="n">time_i</span><span class="p">],</span> <span class="n">Xnn_3layer</span><span class="p">[:</span><span class="n">time_i</span><span class="p">,</span> <span class="n">channel</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;NN 3 layer&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neural_network_for_Lorenz96_58_0.png" src="../_images/Neural_network_for_Lorenz96_58_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Checking over 100 different initial conditions...</span>
<span class="n">err1L</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">err3L</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">T_test</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">init_cond_temp</span> <span class="o">=</span> <span class="n">Xtrue</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">gcm_net_3layers</span> <span class="o">=</span> <span class="n">GCM_network</span><span class="p">(</span><span class="n">Forcing</span><span class="p">,</span> <span class="n">nn_3l</span><span class="p">)</span>
    <span class="n">Xnn_3layer_tmp</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">gcm_net_3layers</span><span class="p">(</span><span class="n">init_cond_temp</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">T_test</span> <span class="o">/</span> <span class="n">dt</span><span class="p">),</span> <span class="n">nn_3l</span><span class="p">)</span>

    <span class="n">gcm_net_1layers</span> <span class="o">=</span> <span class="n">GCM_network</span><span class="p">(</span><span class="n">Forcing</span><span class="p">,</span> <span class="n">lin_net</span><span class="p">)</span>
    <span class="n">Xnn_1layer_tmp</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">gcm_net_1layers</span><span class="p">(</span><span class="n">init_cond_temp</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">T_test</span> <span class="o">/</span> <span class="n">dt</span><span class="p">),</span> <span class="n">lin_net</span><span class="p">)</span>

    <span class="n">err1L</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Xtrue</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">10</span> <span class="p">:</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">T_test</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">Xnn_1layer_tmp</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="n">err3L</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Xtrue</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">10</span> <span class="p">:</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">T_test</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">Xnn_3layer_tmp</span><span class="p">))</span>
    <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum of errors for 1 layer:&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">err1L</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum of errors for 3 layer:&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">err3L</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sum of errors for 1 layer: 54971.36229282367
Sum of errors for 3 layer: 36070.43440283795
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-some-more-to-further-improve-performance">
<h3>Training some more to further improve performance<a class="headerlink" href="#training-some-more-to-further-improve-performance" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Number of epocs</span>
<span class="n">validation_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="c1"># time0 = time()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train_model</span><span class="p">(</span><span class="n">nn_3l</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">))</span>
    <span class="n">validation_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader_test</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">validation_loss</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation loss&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.4408587043468843
validation loss: 2.4213647535523988
train loss: 2.428500583395654
validation loss: 2.405385159083399
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.4357470327151094
validation loss: 2.4044560689621353
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.420948340009739
validation loss: 2.389287121390715
train loss: 2.416438769080365
validation loss: 2.3892660656403595
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.4244442770046337
validation loss: 2.405037598432258
train loss: 2.4083108148786687
validation loss: 2.389884834000205
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.410283625800936
validation loss: 2.394719340928746
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.39795204371508
validation loss: 2.380536194899771
train loss: 2.399532217591545
validation loss: 2.382214012291626
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3960342798560568
validation loss: 2.383437811826909
train loss: 2.395200808643063
validation loss: 2.3705142418785234
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3852695570219717
validation loss: 2.369489818589196
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3871928179757997
validation loss: 2.3725517410078862
train loss: 2.3799743290996282
validation loss: 2.3721497096066537
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.386048625727647
validation loss: 2.3765668689121164
train loss: 2.377760978558799
validation loss: 2.362588752537961
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.378377112626845
validation loss: 2.355645933940844
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3711650215396896
validation loss: 2.357368646630867
train loss: 2.3717977146419713
validation loss: 2.366979911807779
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3735674607832706
validation loss: 2.3855926803704586
train loss: 2.372574521280996
validation loss: 2.3561122803070016
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3624850255490073
validation loss: 2.3714208585082837
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3649363797037783
validation loss: 2.372884335071281
train loss: 2.3647287648486746
validation loss: 2.361980905945559
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3616836919327886
validation loss: 2.3490095451146793
train loss: 2.3584875752596393
validation loss: 2.3561648458291717
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3516062265043174
validation loss: 2.358229746176712
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.346734691765249
validation loss: 2.342416645693179
train loss: 2.345094825746397
validation loss: 2.3498438933862893
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.345456936048083
validation loss: 2.357108937610559
train loss: 2.3460215494037455
validation loss: 2.3429973465658724
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.342429386978328
validation loss: 2.3463013815360516
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3414386351829783
validation loss: 2.3509084211155944
train loss: 2.3401221158972003
validation loss: 2.336948200156332
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.333802624072077
validation loss: 2.3380931075115177
train loss: 2.337894879324235
validation loss: 2.3375296623214648
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.337234026747268
validation loss: 2.339698512369589
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3321143463927823
validation loss: 2.331406080214435
train loss: 2.3355529308757665
validation loss: 2.3394824083970276
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3327059068240357
validation loss: 2.339833299244644
train loss: 2.3327386457296373
validation loss: 2.3348442750833636
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.327918545576475
validation loss: 2.3228830526692796
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.330542786053134
validation loss: 2.3311433815443063
train loss: 2.328974130241215
validation loss: 2.329339280537101
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3267540127254938
validation loss: 2.3367208953650085
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3206352892351485
validation loss: 2.333688027564039
train loss: 2.3194485732502113
validation loss: 2.328281720479672
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3140792108029618
validation loss: 2.3370858916284303
train loss: 2.312097747806184
validation loss: 2.327747649078053
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.320199718138595
validation loss: 2.3312443839706694
train loss: 2.313580960267741
validation loss: 2.327683633409608
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3051576264808764
validation loss: 2.316343218282113
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3057998738507357
validation loss: 2.325201002643615
train loss: 2.304290440037695
validation loss: 2.3230905636255224
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2990677760159692
validation loss: 2.3271583947933197
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.301027221342312
validation loss: 2.3167075359343277
train loss: 2.306520177471024
validation loss: 2.321396691570565
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.296583929696083
validation loss: 2.3120568129340784
train loss: 2.293456195992035
validation loss: 2.3198547004490773
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.293314561566951
validation loss: 2.315991348524232
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.291805921493312
validation loss: 2.3270394900642146
train loss: 2.28918408731107
validation loss: 2.3173964955224187
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2887664612065697
validation loss: 2.3040361954807613
train loss: 2.286187587205157
validation loss: 2.3107836577559437
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2853573473982003
validation loss: 2.3017049637823916
train loss: 2.289296841027923
validation loss: 2.3213164659675174
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.282337942095583
validation loss: 2.30161773287255
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2886766207482436
validation loss: 2.3019458063436815
train loss: 2.2805311927947396
validation loss: 2.308542371302153
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.278155333481558
validation loss: 2.296195379853491
train loss: 2.270331189553331
validation loss: 2.2964142204709996
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2699844896571415
validation loss: 2.2970943552017804
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2674283133371755
validation loss: 2.2974872934788624
train loss: 2.265772660537026
validation loss: 2.303896661322585
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2681946676609934
validation loss: 2.3016662216555854
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2633390253924888
validation loss: 2.2943714897381535
train loss: 2.2633629571938294
validation loss: 2.2903599015561014
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.255213130944466
validation loss: 2.2834698409496066
train loss: 2.264718271580632
validation loss: 2.295919789369503
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.249288355863724
validation loss: 2.276713863039762
train loss: 2.2532060536803127
validation loss: 2.285364810871843
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.242722279014874
validation loss: 2.27721460425547
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.23885939612036
validation loss: 2.2733829266764705
train loss: 2.235599485441473
validation loss: 2.278003434848084
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.24222755696246
validation loss: 2.2930016524601093
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2345977417121965
validation loss: 2.2728562395853498
train loss: 2.228785411457629
validation loss: 2.2726401065982653
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.22978235969004
validation loss: 2.270920309002026
train loss: 2.2292302741159205
validation loss: 2.2646195115491183
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2264625623155903
validation loss: 2.261553639209969
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.218506212531442
validation loss: 2.2606892057084935
train loss: 2.2188501992937617
validation loss: 2.2654442248378173
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2208154957643846
validation loss: 2.2754357501192173
train loss: 2.2198400666871763
validation loss: 2.261092035187028
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.222604676386533
validation loss: 2.2634787319007357
train loss: 2.217198572589525
validation loss: 2.251840944386916
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2164416488814425
validation loss: 2.26603528850364
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.207713735674297
validation loss: 2.2509325997006537
train loss: 2.214824271517468
validation loss: 2.2565728586510443
</pre></div>
</div>
<img alt="../_images/Neural_network_for_Lorenz96_61_60.png" src="../_images/Neural_network_for_Lorenz96_61_60.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save network</span>
<span class="n">PATH</span> <span class="o">=</span> <span class="s2">&quot;network_3_layers_100_epoches.pth&quot;</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">nn_3l</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>

<span class="c1"># Load network</span>
<span class="c1"># path_load = &#39;network_3_layers_100_epoches.pth&#39;</span>
<span class="c1"># nn_3l.load_state_dict(torch.load(path_load))</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="regularization-and-overfitting">
<h1>Regularization and overfitting<a class="headerlink" href="#regularization-and-overfitting" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/overfitting.png&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">700</span><span class="p">)</span>
<span class="c1"># The figure below is taken from Python Machine Learning book by Sebastian Raschka</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neural_network_for_Lorenz96_64_0.png" src="../_images/Neural_network_for_Lorenz96_64_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">validation_loss</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation loss&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neural_network_for_Lorenz96_65_0.png" src="../_images/Neural_network_for_Lorenz96_65_0.png" />
</div>
</div>
<div class="section" id="regularization-methods-are-aimed-to-tackle-overfitting">
<h2>Regularization methods are aimed to tackle overfitting.<a class="headerlink" href="#regularization-methods-are-aimed-to-tackle-overfitting" title="Permalink to this headline">¶</a></h2>
<p>For example, the model on the far right of the plot predicts perfectly on the given set, yet it’s not the best choice. Why is that? If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.</p>
<p>All ML algorithms has some form of regularization.</p>
</div>
<div class="section" id="useful-ways-to-think-of-regularization">
<h2>Useful ways to think of regularization:<a class="headerlink" href="#useful-ways-to-think-of-regularization" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li></li>
</ul>
<div class="section" id="putting-constraints-on-the-model">
<h3>Putting constraints on the model<a class="headerlink" href="#putting-constraints-on-the-model" title="Permalink to this headline">¶</a></h3>
</div>
<p>aiming to have a better generalizability (avoid modeling the noise or ‘’remember’’ training data).</p>
<ul class="simple">
<li></li>
</ul>
<div class="section" id="adding-a-term-to-the-loss-function-so-that-loss-trainingloss-regularization">
<h3>Adding a term to the loss function so that: Loss = TrainingLoss + Regularization<a class="headerlink" href="#adding-a-term-to-the-loss-function-so-that-loss-trainingloss-regularization" title="Permalink to this headline">¶</a></h3>
</div>
<p>put a penalty for making the model more complex.</p>
<p>Very braodly speaking (just to gain intuition) - if we want to reduce the training loss (reduce bias) we should try using a more complex model (if we have enough data) and if we want to reduce overfitting (reduce variace) we should simplify or constraint the model we use (increasing regularization).</p>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="regularization-of-neural-networks">
<h1>Regularization of Neural Networks<a class="headerlink" href="#regularization-of-neural-networks" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Dropout (added in the definition of the network).</p></li>
<li><p>Early stopping</p></li>
<li><p>weight decay (added in the optimizer part - see ?optim.Adam)</p></li>
<li><p>Data augmentation (usually for images)</p></li>
</ul>
<div class="section" id="weight-decay-l2-norm">
<h2>Weight decay (L2 norm)<a class="headerlink" href="#weight-decay-l2-norm" title="Permalink to this headline">¶</a></h2>
<p>weight decay is usually defined as a term that’s added directly to the update rule.
Namely, to update a certain weight $w$ in the $i+1$ iteration, we would use a modified rule:</p>
<p>$w_{i+1} = w_{i} - \gamma ( \frac{\partial L}{\partial w} + A w_{i})$</p>
<p>In practice, this is almost identical to L_2 regularization, though there is some difference (e.g., see <a class="reference external" href="https://bbabenko.github.io/weight-decay/">here</a>)</p>
<p>Weight decay is one of the parameters of the optimizer - try ?torch.optim.SGD</p>
</div>
<div class="section" id="add-a-weight-decay-to-a-network-and-try-to-train-it-again">
<h2>Add a weight decay to a network and try to train it again<a class="headerlink" href="#add-a-weight-decay-to-a-network-and-try-to-train-it-again" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of epocs</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">nn_3l</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.003</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">validation_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="c1"># time0 = time()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train_model</span><span class="p">(</span><span class="n">nn_3l</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">))</span>
    <span class="n">validation_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader_test</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">validation_loss</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation loss&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2431410504998213
validation loss: 2.2853458929863057
train loss: 2.2470188873872106
validation loss: 2.280711157222979
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2597580785565774
validation loss: 2.2922189713081966
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.2764723514954555
validation loss: 2.3136280333309114
train loss: 2.290990025062774
validation loss: 2.3240843045757513
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3005119408301598
validation loss: 2.337053417117639
train loss: 2.305632077995302
validation loss: 2.3331388397723662
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.3123558877316617
validation loss: 2.3327361249297134
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.320111462803631
validation loss: 2.3450140628145593
train loss: 2.3290133773378856
validation loss: 2.3534039377372054
</pre></div>
</div>
<img alt="../_images/Neural_network_for_Lorenz96_72_6.png" src="../_images/Neural_network_for_Lorenz96_72_6.png" />
</div>
</div>
<div class="section" id="dropout">
<h3>Dropout<a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h3>
<p>By dropping a unit out, we mean temporarily removing it from the network while training, along with all its incoming and outgoing connections.
See more details <a class="reference external" href="http://jmlr.org/papers/v15/srivastava14a.html">here</a>.
It is usually the most useful regularization that we can do in fully connected layers</p>
<p>In convolutional layers dropout makes less sense - see more discussion <a class="reference external" href="https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html">here</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Image taken from: http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/Dropout_layer.png&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">700</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neural_network_for_Lorenz96_75_0.png" src="../_images/Neural_network_for_Lorenz96_75_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define network structure in pytorch</span>


<span class="k">class</span> <span class="nc">Net_ANN_dropout</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net_ANN_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>  <span class="c1"># regularization method to prevent overfitting.</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">FF</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">FF</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nn_3l_drop</span> <span class="o">=</span> <span class="n">Net_ANN_dropout</span><span class="p">(</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>  <span class="c1"># Exagerated dropout...</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of epocs</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">nn_3l_drop</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">validation_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="c1"># time0 = time()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train_model</span><span class="p">(</span><span class="n">nn_3l_drop</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l_drop</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">))</span>
    <span class="n">validation_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l_drop</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader_test</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">validation_loss</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation loss&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 28.69650472196512
validation loss: 28.64811442576358
train loss: 25.774482521111036
validation loss: 25.724438868258883
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 23.764734992224064
validation loss: 23.75139409925143
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 21.60552759271213
validation loss: 21.56475438981205
train loss: 20.343701665780905
validation loss: 20.303509551812105
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 18.92113012340524
validation loss: 18.894497739597494
train loss: 18.039393757814583
validation loss: 17.99769771512417
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 17.3136494297489
validation loss: 17.281711910976224
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 16.795758478787242
validation loss: 16.79426893168961
train loss: 16.24088128473153
validation loss: 16.25001112413357
</pre></div>
</div>
<img alt="../_images/Neural_network_for_Lorenz96_77_6.png" src="../_images/Neural_network_for_Lorenz96_77_6.png" />
</div>
</div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="how-to-choose-a-learning-rate">
<h1>How to choose a learning rate?<a class="headerlink" href="#how-to-choose-a-learning-rate" title="Permalink to this headline">¶</a></h1>
<div class="section" id="visuzlization-of-the-loss-function">
<h2>Visuzlization of the loss function<a class="headerlink" href="#visuzlization-of-the-loss-function" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Image taken from: https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/Loss_function_vis_NN.jpeg&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neural_network_for_Lorenz96_80_0.jpg" src="../_images/Neural_network_for_Lorenz96_80_0.jpg" />
</div>
</div>
</div>
<div class="section" id="finding-an-optimal-learning-rate">
<h2>Finding an optimal learning rate<a class="headerlink" href="#finding-an-optimal-learning-rate" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nn_3l_LR</span> <span class="o">=</span> <span class="n">Net_ANN</span><span class="p">()</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">)</span>
<span class="n">lr_finder</span> <span class="o">=</span> <span class="n">LRFinder</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="n">lr_finder</span><span class="o">.</span><span class="n">range_test</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">end_lr</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">lr_finder</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>  <span class="c1"># to inspect the loss-learning rate graph</span>
<span class="n">lr_finder</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># to reset the model and optimizer to their initial state</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "9d3b8b9c0a8341ea800ba3a6ce49562c", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stopping early, the loss has diverged
Learning rate search finished. See the graph with {finder_name}.plot()
LR suggestion: steepest gradient
Suggested LR: 1.29E-02
</pre></div>
</div>
<img alt="../_images/Neural_network_for_Lorenz96_82_2.png" src="../_images/Neural_network_for_Lorenz96_82_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Number of epocs</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">validation_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="c1"># time0 = time()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train_model</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">))</span>
    <span class="n">validation_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader_test</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">validation_loss</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation loss&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 13.817838207034493
validation loss: 13.929791411959432
train loss: 8.226771614476286
validation loss: 8.290345696673075
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 5.637053851147564
validation loss: 5.615243682905206
train loss: 4.111839483694906
validation loss: 4.154723658641473
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 3.470164189380075
validation loss: 3.4714099277549955
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 3.0568650098394725
validation loss: 3.0828191334074604
train loss: 2.9199982855443953
validation loss: 2.95327341762592
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.8615812717760996
validation loss: 2.8854955321817544
train loss: 2.7878157966420636
validation loss: 2.7994535825342193
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.7337502314673996
validation loss: 2.731262905992316
train loss: 2.6933311910757354
validation loss: 2.6852504093349485
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.6587326093219903
validation loss: 2.6400021664837983
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.6226903755637103
validation loss: 2.6029073336323343
train loss: 2.5907269916937157
validation loss: 2.585764176172134
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.5659613946805333
validation loss: 2.554904630845483
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.56006925307251
validation loss: 2.544454986957045
train loss: 2.513115984329809
validation loss: 2.5098973760571974
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.501725426920226
validation loss: 2.505503529876583
train loss: 2.478052964210027
validation loss: 2.4807536345426504
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.4896449054623244
validation loss: 2.5129688180741976
</pre></div>
</div>
<img alt="../_images/Neural_network_for_Lorenz96_83_12.png" src="../_images/Neural_network_for_Lorenz96_83_12.png" />
</div>
</div>
<div class="section" id="we-converged-much-faster-than-before">
<h3>We converged much faster than before.<a class="headerlink" href="#we-converged-much-faster-than-before" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">)</span>
<span class="n">lr_finder</span> <span class="o">=</span> <span class="n">LRFinder</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="n">lr_finder</span><span class="o">.</span><span class="n">range_test</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">end_lr</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">lr_finder</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>  <span class="c1"># to inspect the loss-learning rate graph</span>
<span class="n">lr_finder</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># to reset the model and optimizer to their initial state</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "965d283cda344aaaadaa330e633c2cb6", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stopping early, the loss has diverged
Learning rate search finished. See the graph with {finder_name}.plot()
LR suggestion: steepest gradient
Suggested LR: 9.66E-05
</pre></div>
</div>
<img alt="../_images/Neural_network_for_Lorenz96_85_2.png" src="../_images/Neural_network_for_Lorenz96_85_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of epocs</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">validation_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="c1"># time0 = time()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train_model</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">))</span>
    <span class="n">validation_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_model</span><span class="p">(</span><span class="n">nn_3l_LR</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">loader_test</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">validation_loss</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation loss&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.445316961200866
validation loss: 2.449705032951581
train loss: 2.4404422870062796
validation loss: 2.4451728941227926
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.4369246644499456
validation loss: 2.440761539667167
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.434152922405941
validation loss: 2.43715929774304
train loss: 2.42816224559029
validation loss: 2.4321935796405736
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.4230890155383675
validation loss: 2.4305744534892786
train loss: 2.417524462415921
validation loss: 2.4294636600021526
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.4154916825902397
validation loss: 2.425231101694452
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.4087833818484983
validation loss: 2.4195215532122414
train loss: 2.403620613597289
validation loss: 2.4164180982533723
</pre></div>
</div>
<img alt="../_images/Neural_network_for_Lorenz96_86_6.png" src="../_images/Neural_network_for_Lorenz96_86_6.png" />
</div>
</div>
</div>
</div>
<div class="section" id="i-won-t-talk-about-but-i-recommend-reading">
<h2>I won’t talk about but I recommend reading:<a class="headerlink" href="#i-won-t-talk-about-but-i-recommend-reading" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="batchnormalization">
<h2>BatchNormalization<a class="headerlink" href="#batchnormalization" title="Permalink to this headline">¶</a></h2>
<p>Normalize the activation values such that the hidden representation doesn’t vary drastically and also helps us to get improvement in the training speed.</p>
</div>
<div class="section" id="cyclic-learning-rate">
<h2>Cyclic learning rate<a class="headerlink" href="#cyclic-learning-rate" title="Permalink to this headline">¶</a></h2>
<div class="section" id="to-understand-cyclic-learning-rates-and-the-one-cycle-policy-read-more-here">
<h3>To understand cyclic learning rates and the One cycle policy - read more <a class="reference external" href="https://sgugger.github.io/the-1cycle-policy.html">here</a><a class="headerlink" href="#to-understand-cyclic-learning-rates-and-the-one-cycle-policy-read-more-here" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Image taken from - https://docs.fast.ai/callbacks.one_cycle.html</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;figs/onecycle_params.png&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">700</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neural_network_for_Lorenz96_92_0.png" src="../_images/Neural_network_for_Lorenz96_92_0.png" />
</div>
</div>
<p>To use cyclic learning rates: ?optim.lr_scheduler.CyclicLR</p>
</div>
</div>
</div>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {"0d8e231717ff484cbc8b38562a594f85": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "danger", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_1fd73ab36e7e4c2480b2478610baa474", "max": 200.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_70de7cafe83e407688949298b52fb275", "value": 177.0}}, "0f3ecffcff714ad9aaf28676c8faa4bc": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1517a68881054e6b95474348a7fcd7a1": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1fd73ab36e7e4c2480b2478610baa474": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2520d491c79a4703ba43f0fb94ddfee9": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "34b11f8640ce44098cd77c237567b7cb": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "34f94ff36238455b8089ddc615326da9": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "45ebf4bdc795403bb495a5f6fb507147": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_2520d491c79a4703ba43f0fb94ddfee9", "placeholder": "\u200b", "style": "IPY_MODEL_88e9d7e3cbd64983aebbccf4794d9cad", "value": " 177/200 [00:01&lt;00:00, 166.36it/s]"}}, "5a8202167c374a06a14b56302731f278": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "danger", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_1517a68881054e6b95474348a7fcd7a1", "max": 200.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_bd33e81eac084088980667e1a8eb891f", "value": 151.0}}, "70de7cafe83e407688949298b52fb275": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "82ff5bfa867540a8ae24a4f859a59dc4": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "88e9d7e3cbd64983aebbccf4794d9cad": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "8a47b6d6800345cb88d864f444449dd3": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_34f94ff36238455b8089ddc615326da9", "placeholder": "\u200b", "style": "IPY_MODEL_c4f07c0acdc44a9fb2e90eeb175f7284", "value": " 76%"}}, "965d283cda344aaaadaa330e633c2cb6": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_8a47b6d6800345cb88d864f444449dd3", "IPY_MODEL_5a8202167c374a06a14b56302731f278", "IPY_MODEL_a3a012c75bfb4558b28ff3a4f9afbf03"], "layout": "IPY_MODEL_34b11f8640ce44098cd77c237567b7cb"}}, "9d3b8b9c0a8341ea800ba3a6ce49562c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_f12537366e064427946121b055d1a932", "IPY_MODEL_0d8e231717ff484cbc8b38562a594f85", "IPY_MODEL_45ebf4bdc795403bb495a5f6fb507147"], "layout": "IPY_MODEL_a82531ae163840b2b1d62edf3bde91db"}}, "a3a012c75bfb4558b28ff3a4f9afbf03": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_0f3ecffcff714ad9aaf28676c8faa4bc", "placeholder": "\u200b", "style": "IPY_MODEL_82ff5bfa867540a8ae24a4f859a59dc4", "value": " 151/200 [00:00&lt;00:00, 179.96it/s]"}}, "a82531ae163840b2b1d62edf3bde91db": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "bd33e81eac084088980667e1a8eb891f": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "bdeb34414bb04edca039c72acfc75201": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c4f07c0acdc44a9fb2e90eeb175f7284": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "d6fb31849cf948acbacb56f0e3a80942": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "f12537366e064427946121b055d1a932": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_bdeb34414bb04edca039c72acfc75201", "placeholder": "\u200b", "style": "IPY_MODEL_d6fb31849cf948acbacb56f0e3a80942", "value": " 88%"}}}, "version_major": 2, "version_minor": 0}
</script>


    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="DA_demo_L96.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Data Assimilation demo in the Lorenz 96 (L96) two time-scale model</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="gradient_decent.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Neural networks</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The M2LinES Community<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>