{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45486967",
   "metadata": {},
   "source": [
    "# Estimating model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686988e8",
   "metadata": {},
   "source": [
    "*The objective of this notebook is to show how GCM closures can be tuned in practice. We will assume a specific formulation of a closure and estimate its parameters through a standard optimisation procedure with DA.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1f6aa",
   "metadata": {},
   "source": [
    "**Resources** : We have used material from Emmanuel Cosme's nice GitHub repos : https://github.com/ecosme38/Data-Assimilation-Notebooks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa836f4",
   "metadata": {},
   "source": [
    "## 1. Copy / pasting from gcm-parameterization-problem notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a03d0b",
   "metadata": {},
   "source": [
    "Starting from `gcm-parameterization-problem`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ac7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# L96 provides the \"real world\", L96_eq1_xdot is the beginning of rhs of X tendency\n",
    "from L96_model import L96, RK2, RK4, EulerFwd, L96_eq1_xdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed gives us reproducible results\n",
    "np.random.seed(13)\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "W = L96(8, 32, F=18)\n",
    "\n",
    "# Run \"real world\" for 3 days to forget initial conditons\n",
    "# (store=True save the final state as an initial condition for the next run)\n",
    "W.run(0.05, 3.0, store=True);\n",
    "\n",
    "# From here on we can use W.X as perfect initial conditions for a model\n",
    "# and sample the real world using W.run(dt,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM:\n",
    "    def __init__(self, F, parameterization, time_stepping=EulerFwd):\n",
    "        self.F = F\n",
    "        self.parameterization = parameterization\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        return L96_eq1_xdot(X, self.F) - self.parameterization(param, X)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18baf519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a first step, we illustrate introducing a polynomial parameterization to GCM\n",
    "naive_parameterization = lambda param, X: np.polyval(param, X)\n",
    "F, dt, T = 18, 0.01, 5.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "X, t = gcm(W.X, dt, int(T / dt), param=[0.85439536, 1.75218026])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d260623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing model and true trajectories.\n",
    "\n",
    "# This samples the real world with the same time interval as \"dt\" used by the model\n",
    "Xtrue, _, _ = W.run(dt, T)\n",
    "\n",
    "plt.plot(t, Xtrue[:, 4], label=\"Truth\")\n",
    "plt.plot(t, X[:, 4], label=\"Model\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b2947",
   "metadata": {},
   "source": [
    "We also copy the distance metrics used in the gcm-parameterization-problem notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8989a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - pointwise distance :\n",
    "def pointwise(X1, X2, L=1.0):  # computed over some window t<L.\n",
    "    D = (X1 - X2)[np.where(t < L)]\n",
    "    return np.sqrt(D**2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca2ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - mean state metric :\n",
    "def dist_mean(X1, X2, L=1.0):\n",
    "    _X1 = X1[np.where(t < L)]\n",
    "    _X2 = X2[np.where(t < L)]\n",
    "    return np.sqrt((_X1.mean(axis=0) - _X2.mean(axis=0)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_initial_tendency(X1, X2):\n",
    "    T1 = X1[1, :] - X1[0, :]\n",
    "    T2 = X2[1, :] - X2[0, :]\n",
    "    return np.sqrt((T1 - T2) ** 2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3200004c",
   "metadata": {},
   "source": [
    "## 2. Variational estimation of optimal parameters for a predefined closure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bf060",
   "metadata": {},
   "source": [
    "We will try here to estimate the parameters of `naive_parameterization` with a variational approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5141f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - assuming the formulation of the parameterization\n",
    "gcm = GCM(F, naive_parameterization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b14903",
   "metadata": {},
   "source": [
    "### 2.1 Estimating parameters based on one initial condition and one time step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57669f4",
   "metadata": {},
   "source": [
    "#### Cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91466a50",
   "metadata": {},
   "source": [
    "What we will be doing here is very close to what is done with classical variational data assimilation, where people try to estimate the state of the parameters of a model through the minimization of a cost function $J$. This is also very close to what is done when parameterizations are encoded as neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9ab28",
   "metadata": {},
   "source": [
    "We introduce a cost function $J(p)$ which depends on the parameters of the closure. \n",
    "\n",
    "$$J(p) = ||X_p - X_{true}||_{d}$$\n",
    "\n",
    "where $p=[p1,p2]$, $X_p$ is GCM solution computed with with parameters $p$ and $||\\cdot ||_{d}$ is one of the distances above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce939e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(param):\n",
    "    F, dt, T = 18, 0.01, 0.01\n",
    "    Xgcm, t = gcm(W.X, dt, int(T / dt), param=param)\n",
    "    return norm_initial_tendency(Xtrue, Xgcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6198df7",
   "metadata": {},
   "source": [
    "#### Minimization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d38b2b",
   "metadata": {},
   "source": [
    "The problem dimension being small enough (2 parameters to find), one can use efficient derivative-free optimization methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "prior = np.array([0.85439536, 1.75218026])  #  prior\n",
    "res = opt.minimize(cost_function, prior, method=\"Powell\")\n",
    "opt_param = res[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdffeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Let's test our closure\n",
    "F, dt, T = 18, 0.01, 100.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "Xopti, t = gcm(W.X, dt, int(T / dt), param=opt_param)\n",
    "Xprior, t = gcm(W.X, dt, int(T / dt), param=prior)\n",
    "\n",
    "# - ... the true state\n",
    "Xtrue, _, _ = W.run(dt, T)\n",
    "\n",
    "## and plot the results\n",
    "plt.plot(t[:500], Xtrue[:500, 4], label=\"Truth\")\n",
    "plt.plot(t[:500], Xprior[:500, 4], label=\"initial GCM\")\n",
    "plt.plot(t[:500], Xopti[:500, 4], label=\"optimized GCM\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc951db",
   "metadata": {},
   "source": [
    "The reult of our model with the new optimized parameters is better, but still requires further improvements... This problem is related to the question of *a priori* versus *a posteriori* skill in LES closures.\n",
    "The optimization we have done here tries to minimize the difference between the true (L96) model to the parameterized GCM (that does not explicitly resolves the small-scale degrees of freedom) over a single time step. This in many ways similar to how some machine learning parameterizations are currently trained {cite:p}`yuval2020stable,bolton2019applications` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de43a461",
   "metadata": {},
   "source": [
    "### 2.2 Estimating parameters which optimize longer trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.01, 5.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "Xtrue, _, _ = W.run(dt, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgcm, t = gcm(W.X, dt, int(T / dt), param=[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(param):\n",
    "    F, dt, T = 18, 0.01, 5\n",
    "    Xgcm, t = gcm(W.X, dt, int(T / dt), param=param)\n",
    "    return pointwise(Xtrue, Xgcm, L=5.0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b152335",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.array([0.85439536, 1.75218026])  #  prior\n",
    "res = opt.minimize(cost_function, prior, method=\"Powell\")\n",
    "opt_param = res[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Let's test our closure\n",
    "F, dt, T = 18, 0.01, 100.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "Xopti, t = gcm(W.X, dt, int(T / dt), param=opt_param)\n",
    "Xprior, t = gcm(W.X, dt, int(T / dt), param=prior)\n",
    "\n",
    "# - ... the true state\n",
    "Xtrue, _, _ = W.run(dt, T)\n",
    "\n",
    "## and plot the results\n",
    "plt.plot(t[:500], Xtrue[:500, 4], label=\"Truth\")\n",
    "plt.plot(t[:500], Xprior[:500, 4], label=\"initial GCM\")\n",
    "plt.plot(t[:500], Xopti[:500, 4], label=\"optimized GCM\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47764aff",
   "metadata": {},
   "source": [
    "This approach is close to using an online learning scheme which tries to optimize the prediction over several time steps. Research suggest that such an online learning scheme might work better (and would help to tackle instabilities and biases in hybrid climate models) when running online simulaitons {cite:p}`bar2019learning`. \n",
    "\n",
    "Unfortunately, using an online learning scheme to tune climate models or to develop parameterizations would be a very difficult task since it would require having a differentiable code, but possibly other  online learning schemes (that won't require differentiable code) could be developed {cite:p}`rasp2020coupled`.\n",
    "\n",
    "<!-- This result is better but it is not clear how this would generalize to unseen initial conditions.  -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2b351",
   "metadata": {},
   "source": [
    "### Comment for Julien: Below is Janni's an attempt to make optimization over multiple steps. (seems not to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This samples the real world with the same time interval as \"dt\" used by the model\n",
    "init_cond_len = 1000\n",
    "Xtrue_list = list()\n",
    "for i in range(init_cond_len):\n",
    "    W1 = L96(8, 32, F=18)\n",
    "    Xtrue_list.append(W1.run(dt, dt)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_long(param, init_cond_len=1000):\n",
    "    F, dt, T = 18, 0.01, 0.01\n",
    "    cost = 0\n",
    "    for i in range(init_cond_len):\n",
    "        Xgcm, t = gcm(Xtrue_list[i][0, :], dt, int(T / dt), param=param)\n",
    "        cost = cost + norm_initial_tendency(Xtrue_list[i], Xgcm)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.array([0.85439536, 1.75218026])  #  prior\n",
    "res = opt.minimize(cost_function_long, prior, method=\"Powell\")\n",
    "opt_param = res[\"x\"]\n",
    "\n",
    "# - Let's test our closure\n",
    "F, dt, T = 18, 0.01, 10.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "Xopti, t = gcm(W.X, dt, int(T / dt), param=opt_param)\n",
    "Xprior, t = gcm(W.X, dt, int(T / dt), param=prior)\n",
    "# Xnop, t = gcm(W.X, dt, int(T / dt), param=[0,0])\n",
    "\n",
    "# - ... the true state\n",
    "Xtrue, _, _ = W.run(dt, T)\n",
    "\n",
    "## and plot the results\n",
    "plt.plot(t[:500], Xtrue[:500, 4], label=\"Truth\")\n",
    "plt.plot(t[:500], Xprior[:500, 4], label=\"initial GCM\")\n",
    "plt.plot(t[:500], Xopti[:500, 4], label=\"optimized GCM\")\n",
    "# plt.plot(t[:500], Xnop[:500, 4], label=\"no param\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4039f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
